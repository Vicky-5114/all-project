{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de85f07f1227a50",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Step 1: 深度学习核心流程总览**\n",
    "任何深度学习项目都包含以下步骤：\n",
    "1. **数据准备**：加载、预处理、划分数据集\n",
    "2. **模型构建**：定义神经网络结构\n",
    "3. **训练配置**：选择损失函数和优化器\n",
    "4. **训练循环**：前向传播 → 计算损失 → 反向传播 → 更新参数\n",
    "5. **模型评估**：在测试集上验证性能\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: 代码实现与逐行解析**\n",
    "#### **2.1 数据准备**\n",
    "```python\n",
    "# 导入必要库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "# 加载MNIST数据集（CSV格式）\n",
    "train_data = pd.read_csv(\"mnist_train.csv\", header=None)\n",
    "test_data = pd.read_csv(\"mnist_test.csv\", header=None)\n",
    "\n",
    "# 数据预处理：分离特征和标签 + 归一化\n",
    "x_train = train_data.iloc[:, 1:].values / 255.0  # 归一化到[0,1]\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "x_test = test_data.iloc[:, 1:].values / 255.0\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器（方便批量训练）\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), \n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test, y_test), \n",
    "                        batch_size=batch_size)\n",
    "```\n",
    "\n",
    "**关键解释**：\n",
    "- `归一化 /255.0`：将像素值从0-255缩放到0-1，这是神经网络的常见要求\n",
    "- `DataLoader`：将数据打包成批次，`shuffle=True`让每批数据顺序随机\n",
    "- `TensorDataset`：将特征和标签打包成PyTorch标准数据集格式\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.2 模型构建**\n",
    "```python\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 128)  # 输入784维，输出128维\n",
    "        self.layer2 = nn.Linear(128, 10)   # 输出10维（对应10个数字）\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  # 激活函数\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "print(model)  # 打印网络结构\n",
    "```\n",
    "\n",
    "**输出示例**：\n",
    "```\n",
    "SimpleNN(\n",
    "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
    "  (layer2): Linear(in_features=128, out_features=10, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "**关键概念**：\n",
    "- `nn.Linear`：全连接层，计算 `y = Wx + b`\n",
    "- `ReLU`：激活函数，引入非线性能力\n",
    "- 输入层784 → 隐藏层128 → 输出层10的经典结构\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.3 训练配置**\n",
    "```python\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 分类任务常用损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 自适应学习率优化器\n",
    "\n",
    "epochs = 5  # 训练轮数（可根据算力调整）\n",
    "```\n",
    "\n",
    "**为什么选择这些组件**：\n",
    "- `CrossEntropyLoss`：直接处理分类任务的概率输出\n",
    "- `Adam`：比传统SGD优化器更智能，自动调整学习率\n",
    "- 学习率`lr=0.001`：常用初始值，太大容易震荡，太小收敛慢\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.4 训练循环**\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 设置为训练模式\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:  # 遍历所有批次\n",
    "        optimizer.zero_grad()  # 清空梯度（重要！）\n",
    "        \n",
    "        outputs = model(inputs)  # 前向传播\n",
    "        loss = criterion(outputs, labels)  # 计算损失\n",
    "        loss.backward()          # 反向传播\n",
    "        optimizer.step()         # 更新参数\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "```\n",
    "\n",
    "**训练过程输出示例**：\n",
    "```\n",
    "Epoch [1/5], Loss: 0.3521\n",
    "Epoch [2/5], Loss: 0.1824\n",
    "Epoch [3/5], Loss: 0.1327\n",
    "Epoch [4/5], Loss: 0.1048\n",
    "Epoch [5/5], Loss: 0.0873\n",
    "```\n",
    "\n",
    "**关键操作**：\n",
    "- `zero_grad()`：防止梯度累积（必须每次清空）\n",
    "- `loss.backward()`：自动计算所有参数的梯度\n",
    "- `optimizer.step()`：根据梯度更新参数\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.5 模型评估**\n",
    "```python\n",
    "model.eval()  # 设置为评估模式\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算（节省内存）\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 取概率最高的类别\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "```\n",
    "\n",
    "**输出示例**：\n",
    "```\n",
    "Test Accuracy: 97.23%\n",
    "```\n",
    "\n",
    "**结果解读**：\n",
    "- 即使简单的网络也能达到97%+准确率\n",
    "- MNIST是相对简单的数据集\n",
    "- 实际复杂任务需要更深的网络\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: 代码能力提升技巧**\n",
    "#### **技巧1：理解张量形状**\n",
    "在关键位置插入形状检查：\n",
    "```python\n",
    "print(f\"输入形状: {inputs.shape}\")  # 应显示 [64, 784]\n",
    "print(f\"输出形状: {outputs.shape}\") # 应显示 [64, 10]\n",
    "```\n",
    "\n",
    "#### **技巧2：可视化训练过程**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 在训练循环中记录损失\n",
    "loss_history = []\n",
    "...\n",
    "loss_history.append(avg_loss)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **技巧3：调试梯度问题**\n",
    "```python\n",
    "# 检查梯度是否存在\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}的梯度：\", param.grad is not None)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: 常见问题解答**\n",
    "#### **Q1: 为什么要用`model.train()`和`model.eval()`？**\n",
    "- `train()`模式：启用Dropout/BatchNorm等训练专用层\n",
    "- `eval()`模式：关闭这些层，保证推理一致性\n",
    "\n",
    "#### **Q2: 如何选择隐藏层大小？**\n",
    "- 经验法则：前一层大小的1/2到1/4\n",
    "- 示例：784 → 256 → 64 → 10\n",
    "- 可以通过实验调整（尝试128/256/512等）\n",
    "\n",
    "#### **Q3: 遇到内存不足怎么办？**\n",
    "- 减小`batch_size`（如从64改为32）\n",
    "- 简化网络结构\n",
    "- 使用`torch.no_grad()`减少内存占用\n",
    "\n",
    "---\n",
    "\n",
    "### **下一步学习建议**\n",
    "1. **修改网络结构**：尝试添加更多隐藏层，观察准确率变化\n",
    "2. **调整超参数**：改变学习率、batch_size、epochs\n",
    "3. **实现图像显示**：用matplotlib显示预测错误的图片\n",
    "4. **进阶技巧**：添加卷积层（CNN）、使用数据增强\n",
    "\n",
    "需要任何环节的深入解释，或者遇到具体问题，请随时告诉我！我们可以针对某个具体部分进行更详细的探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af13a010db357a56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f5271a93513ee6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
